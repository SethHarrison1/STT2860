# Interpreting regression models

This chapter looks at how to interpret the coefficients in a regression model.

<iframe src="https://drive.google.com/file/d/1E0KgjNlN12xw8piCIqq_mOsU_SY4V5Gy/preview" width="740" height="420"></iframe>

### Interpretation of coefficients{-}

Recall that the fitted model for the poverty rare of U.S. countries as a function of high school graduation rate is:

\begin{equation}
\widehat{poverty} = 64.594 - 0.591\cdot {hs\_grad}
\end{equation}

Which of the following equations is the correct interpretation of the slope coefficient?

* Among U.S. counties, each additional percentage point increase in the poverty rate is associated with about a 0.591 percentage point decrease in the high school graduation rate.
* **Among U.S. counties, each additional percentage point increase in the high school graduation rate is associated with about a 0.591 percentage point decrease in the poverty rate.**
* Among U.S. counties, each additional percentage point increase in the high school graduation rate is associated with about a 0.591 percentage point increase in the poverty rate.
* Among U.S. counties, a 1% increase in the high school graduation rate is associated with about a 0.591% decrease in the poverty rate.

---------

### Interpretation in context{-}

A politician interpreting the relationship between poverty rates and high school graduation rates implores his constituents:

If we can lower the poverty rate by 59%, we’ll double the high school graduate rate in our county (i.e. raise it by 100%).

Which of the following mistakes in interpretation has the politician made?

* Implying that the regression model establishes a cause-and-effect relationship.

* Switching the role of the response and explanatory variables.

* Confusing percentage change with percentage point change.

* **All of the above.**

* None of the above.

--------

## Fitting simple linear models

While the `geom_smooth(method = "lm")` function is useful for drawing linear models on a scatterplot, it doesn’t actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is `lm()`. This function generally takes two arguments:

* A `formula` that specifies the model

* A `data` argument for the data frame that contains the data you want to fit to the model

The `lm()` function return a model object having class `"lm"`. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc.

--------

### Excercise{-}

* Using the `bdims` dataset, create a linear model for the weight of people as a function of their height.

```{r, message=FALSE}
library(openintro)
# Linear model for weight as a function of height
lm(wgt ~ hgt, data = bdims)
```

* Using the `mlbBat10` dataset, create a linear model for `SLG` as a function of `OBP`.

```{r}
# Linear model for SLG as a function of OBP
lm(slg ~ obp, data = mlbbat10)
```

* Using the `mammals` dataset, create a linear model for the body weight of mammals as a function of their brain weight, after taking the natural log of both variables.

```{r}
# Log-linear model for body weight as a function of brain weight
lm(log(body_wt) ~ log(brain_wt), data = mammals)
```

--------

### Units and scale{-}

In the previous examples, we fit two regression models:


\begin{equation}
\widehat{wgt} = −105.011 + 1.018 \cdot hgt
\end{equation}

and

\begin{equation}
\widehat{SLG}= 0.009 + 1.110 \cdot OBP
\end{equation}

Which of the following statements is **incorrect**?

* A person who is 170 cm tall is expected to weigh about 68 kg.

* **Because the slope coefficient for OBP is larger (1.110) than the slope coefficient for hgt (1.018), we can conclude that the association between OBP and SLG is stronger than the association between height and weight.**

* None of the above.

--------

<iframe src="https://drive.google.com/file/d/1O_H-m52pJWTgG3zss1jq387n0FN-Wc1-/preview" width="740" height="420"></iframe>

## The lm summary output

An `lm` object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.

The `coef()` function displays only the values of the coefficients. Conversely, the `summary()` function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the, $R^2$ adjusted $R^2$, and the residual standard error. The summary of an `"lm"` object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)

-------

### Excercise{-}

We have already created the mod object, a linear model for the weight of individuals as a function of their height, using the `bdims` dataset and the code

```{r, results='hide'}
mod <- lm(wgt ~ hgt, data = bdims)
```

Now, you will:

* Use `coef()` to display the coefficients of `mod`.

```{r}
mod <- lm(wgt ~ hgt, data = bdims)
# Show the coefficients
coef(mod)
```
* Use `summary()` to display the full regression output of `mod()`.

```{r}
summary(mod)
```

-------

## Fitted values and residuals

Once you have fit a regression model, you are often interested in the fitted values $(\hat{y_i})$ and the residuals $({e_i})$, where $i$ indexes the observations. Recall that:
\begin{equation}
e_i = y_i - \hat{y}_i
\end{equation}
The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable.

In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the `fitted.values()` and `residuals()` functions, respectively, for the following model:
```{r}
mod <- lm(wgt ~ hgt, data = bdims)
```

-------

### Exercise{-}

* Confirm that the mean of the body weights equals the mean of the fitted values of `mod`.

```{r}
# Mean of weights equal to mean of fitted values?
mean(bdims$wgt) == mean(fitted.values(mod))
```

* Compute the mean of the residuals of `mod`.

```{r}
mean(resid(mod))
```

-------

## Tidying your linear model

As you fit a regression model, there are some quantities (e.g. $R^2$) hat apply to the model as a whole, while others apply to each observation (e.g. $\hat{y_i}$). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.

The `augment()` function from the `broom` package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.

-------

### Excercise{-}

The same linear model from the last exercise, `mod`, is available in your workspace.

* Load the `broom` package.

```{r}
# Load broom
library(broom)
```
* Create a new data frame called `bdims_tidy` that is the augmentation of the `mod` linear model.

```{r}
# Create bdims_tidy
bdims_tidy <- augment(mod)
```

* View the `bdims_tidy` data frame using `glimpse()`.

```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
```


```{r}
# Glimpse the resulting data frame
glimpse(bdims_tidy)
```

-------

<iframe src="https://drive.google.com/file/d/1ucs1U-6mmEm-KybnZ-b-cyWVr3gIC6ZA/preview" width="740" height="420"></iframe>

## Making predictions

The `fitted.values()` function or the `augment()`-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were not present in the data on which the model was fit. These types of predictions are called out-of-sample.

The `ben` data frame contains a height and weight observation for one person.

```{r}
ben <- data.frame(wgt = 74.8, hgt = 182.8)
```

The `mod` object contains the fitted model for weight as a function of height for the observations in the `bdims` dataset. We can use the `predict()` function to generate expected values for the weight of new individuals. We must pass the data frame of new observations through the `newdata` argument.

-------

### Excercise{-}

The same linear model, `mod`, is defined in your workspace.

* Print `ben` to the console.

```{r}
# Print ben
ben
```

* Use `predict()` with the `newdata` argument to compute the expected weight of the individual in the `ben` data frame.

```{r}
# Predict the weight of ben
predict(mod, newdata = ben)
```

**Note** that the data frame `ben` has variables with the exact same names as those in the fitted model.

-------

## Adding a regression line to a plot manually

The `geom_smooth()` function makes it easy to add a simple linear regression line to a scatterplot of the corresponding variables. And in fact, there are more complicated regression models that can be visualized in the data space with `geom_smooth()`. However, there may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the `geom_abline()` function, which takes slope and intercept arguments. Naturally, we have to compute those values ahead of time, but we already saw how to do this (e.g. using `coef()`).

The `coefs` data frame contains the model estimates retrieved from `coef()`. Passing this to `geom_abline()` as the data argument will enable you to draw a straight line on your scatterplot.

-------

### Excercise{-}

Use `geom_abline()` to add a line defined in the `coefs` data frame to a scatterplot of weight vs. height for individuals in the `bdims` dataset.

```{r}
coef(mod)
```

```{r}
coefs <- data.frame(intercept = -105, slope = 1.018)
coefs
```

```{r}
# Add the line to the scatterplot
ggplot(data = bdims, aes(x = hgt, y = wgt)) +
  geom_point(alpha = 0.5) + 
  theme_bw() + 
  geom_abline(data = coefs, 
              aes(intercept = intercept, slope = slope),  
              color = "dodgerblue")
```

-------