---
title: "Credit Card Customer Retention"
author: "Seth Harrison"
output: bookdown::html_document2
bibliography: [packages.bib]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, fig.align = "center")
```


```{r, include=FALSE, message=FALSE, warning=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'tidyverse', 'base', 'caret', 'tidyr', 'dplyr', 'ggplot2', 'cowplot', 'h20'
), 'packages.bib')
library(dplyr)
library(tidyr)
library(caret)
library(scales)
library(gridExtra)
library(h2o)
library(cowplot)
library(ggplot2)
```

```{r, include=FALSE}
creditData <- read.csv("./BankChurners.csv")
creditData <- creditData[,1:21] ## Removal of two classifier variables that are used for bayesian modeling.
creditData <- na.omit(creditData) ## Removal of one missing row
```
# Introduction {-}
Given customer data from a bank who has seen poor retention of members of one of their credit card programs, we want to be able to predict who will likely be churning, or leaving the program. I acquired this dataset from https://leaps.analyttica.com/home though I am not sure where they originally acquired it. My primary goal is to identify a well trained model that will yield accurate results on testing data for predicting whether a given customer will leave the banks credit cards program.

# First Impressions of the Data{-}
We have a dataset consisting of 10,000 customers with 21 variables, including whether or not they left the program. It is important first to understand what our variables are to get an idea of what may be useful for predicting churn rates. Our variables are as follows:


* CLIENTNUM: Client number used to identify the holders account
* Attrition_Flag: Indicative of whether the account has been closed, 1 for closed and 0 for open
* Customer_Age: Customer's Age in Years
* Gender: M=Male, F=Female
* Dependent_count: Number of dependents
* Education_Level: Educational Qualification of the account holder (ex. high school, college graduate)
* Marital_Status: Married, Single, Divorced, Unknown
* Income_Category: Annual Income Range of the account holder (<  40K, 40K - 60K,  60K− 80K,  80K− 120K, > $120K, Unknown)
* Card_Category: Type of Card (Blue, Silver, Gold, Platinum)
* Months_on_book: Period of relationship with bank
* Total_Relationship_Count: Total number of products held by the customer
* Months_Inactive_12_mon: Number of months inactive in the last 12 months
* Contacts_Count_12_mon: Number of Contacts in the last 12 months
* Credit_Limit: Credit Limit on the holders Credit Card
* Total_Revolving_Bal: Total Revolving Balance on the holders Credit Card
* Avg_Open_To_Buy: Open to Buy Credit Line (12 Month Average)
* Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Between Q4 and Q1)
* Total_Trans_Amt: Total Transaction Amount (Last 12 months)
* Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Between Q4 and Q1)
* Avg_Utilization_Ratio: Average Card Utilization Ratio

At a glance, it appears as if the total transaction amounts, total transaction counts, and utilization ratio would be some of the stronger indicators of whether or not a customer might close their account. There are other potentially strongly correlated variables as well though and there may be something about this particular credit card program that we cannot see from just the data set that might prove to be a good indicator. Below is a comparison between the current customer's and former customer's total transaction amounts and total transaction counts.
```{r, fig.width=10,fig.align='center',echo=FALSE}
ggplot(creditData, aes(x = Total_Trans_Ct, y = Total_Trans_Amt, col = Attrition_Flag)) + geom_point() + theme_bw() + labs(title = "Comparison of Transaction Counts and Amounts Between Former and Current Customers" , x = "Total Transaction Count", y = "Total Transaction Amount")
```
Based on the comparison of these two variables, it seems fairly likely that one if not both of them may be included in our final model. There is a large subset of individuals above both a certain level of both total transaction count and amount that does not include any attrited customers. We should also try to get an idea of the type of customers included in our dataset.
```{r, fig.width=12, fig.height=12, echo=FALSE}
mycols <- c("#e7b794", "#d39840")
attTable <- table(creditData$Attrition_Flag)
count.data <- data.frame(
  class= c("Attrited", "Existing"),
  n = c(attTable[2], attTable[1]),
  prop = c(round(attTable[1]/9831 * 100, 2), round(attTable[2]/9831 * 100, 2)))
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = round(cumsum(prop) - 0.5*prop), 2)
attPie <- ggplot(count.data, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", col = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual("Attrition Flag", values = mycols) +
  theme_void() + labs(title = "Customer Account Status")

mycols <- c("#e7b794", "#d39840")
genderTable <- table(creditData$Gender)
count.data <- data.frame(
  class= c("Female", "Male"),
  n = c(genderTable[1], genderTable[2]),
  prop = c(round(genderTable[1]/9831 * 100, 2), round(genderTable[2]/9831 * 100, 2)))
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = round(cumsum(prop) - 0.5*prop), 2)
genderPie <- ggplot(count.data, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", col = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual("Gender", values = mycols) +
  theme_void() + labs(title = "Customer Gender")

mycols <- c("#e7b794", "#d39840", "#30424a", "#e5813a")
maritalTable <- table(creditData$Marital_Status)
count.data <- data.frame(
  class= c("Divorced", "Married", "Single", "Unknown"),
  n = c(maritalTable[1], maritalTable[2], maritalTable[3], maritalTable[4]),
  prop = c(round(maritalTable[1]/9831 * 100, 2), round(maritalTable[2]/9831 * 100, 2), round(maritalTable[3]/9831 * 100, 2), round(maritalTable[4]/9831 * 100, 2)))
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = round(cumsum(prop) - 0.5*prop), 2)
maritalPie <- ggplot(count.data, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", col = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual("Status", values = mycols) +
  theme_void() + labs(title = "Customer Marital Status")

mycols <- c("#e7b794", "#d39840", "#30424a", "#e5813a", "#f4f1e9", "#f3e5d4", "#cae7c1")
incomelTable <- table(creditData$Income_Category)
count.data <- data.frame(
  class= c("$120K +", "$40K - $60K", "$60K - $80K", "$80K - $120K", "Less than", "Less than $40K", "Unknown"),
  n = c(incomelTable[1], incomelTable[2], incomelTable[3], incomelTable[4], incomelTable[5], incomelTable[6], incomelTable[7]),
  prop = c(round(incomelTable[1]/9831 * 100, 2), round(incomelTable[2]/9831 * 100, 2), round(incomelTable[3]/9831 * 100, 2),
           round(incomelTable[4]/9831 * 100,2),round(incomelTable[5]/9831 * 100, 2), round(incomelTable[6]/9831 * 100, 2),
           round(incomelTable[7]/9831 * 100,2))
  )
count.data <- count.data %>%
  arrange(desc(class)) %>%
  mutate(lab.ypos = round(cumsum(prop) - 0.5*prop), 2)
incomePie <- ggplot(count.data, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", col = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = lab.ypos, label = prop), color = "white")+
  scale_fill_manual("Income", values = mycols) +
  theme_void() + labs(title = "Customer Income Level")

grid.arrange(attPie, genderPie, maritalPie, incomePie, ncol = 2)
```
Looking at the some of the variables that classify the types of customers in our dataset, it appears we have a slight majority of women over men at around 53%. Only about 16% of the accounts in our data set are attrited, the majority of members are married or single, and most customers make less than $40,000 dollars a year. There are a few other qualifying variables that we can look at but this just gives us an initial look at the types of customers in the data set and may be useful to know when we begin to develop models.

Before we begin to build a model it might be helpful to take a look at some of our predictors split by whether the customer has attrited.

```{r, fig.width=12, fig.height=8, echo=FALSE}
relationData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Total_Relationship_Count))
relationBar <- ggplot(relationData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#e5813a") + labs(y = "", x = "", title = "Average Total Relationships") + theme_bw() + background_grid(color.major = "#FFFFFF")

activityData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Months_Inactive_12_mon))
activityBar <- ggplot(activityData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#e7b794") + labs(y = "", x = "", title = "Average Months Inactive") + theme_bw()+ background_grid(color.major = "#FFFFFF")

balanceData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Total_Revolving_Bal))
balanceBar <- ggplot(balanceData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#d39840") + labs(y = "", x = "", title = "Average Revolving Balance") + theme_bw()+ background_grid(color.major = "#FFFFFF")

transAmountData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Total_Trans_Amt))
transAmountBar <- ggplot(transAmountData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#cae7c1") + labs(y = "", x = "", title = "Average Transaction Amount") + theme_bw()+ background_grid(color.major = "#FFFFFF")

transCountData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Total_Trans_Ct))
transCountBar <- ggplot(transCountData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#f3e5d4") + labs(y = "", x = "", title = "Average Transaction Count") + theme_bw()+ background_grid(color.major = "#FFFFFF")

utilizationData <- creditData %>% group_by(Attrition_Flag) %>% summarise(me = mean(Avg_Utilization_Ratio))
utilizationnBar <- ggplot(utilizationData, aes(x = Attrition_Flag, y = me)) + geom_bar(stat = "identity", fill = "#30424a") + labs(y = "", x = "", title = "Average Utilization Ratio") + theme_bw()+ background_grid(color.major = "#FFFFFF")

grid.arrange(relationBar, activityBar, balanceBar, transAmountBar,
             transCountBar, utilizationnBar, ncol = 3)
```

This does not show every variable that could be analyzed this way, but ones that appeared to have potential significance when looking at the data. We can see that the average total relationships of an existing customer is higher than that of an attrited. The same goes for the average utilization ratio, transaction count, revolving balance, and transaction amount. We also see that the average months inactive is greater for an attrited customer, but only slightly. Some of these seem obvious, though we now have an idea just how much of a difference there is between them. 


# Modelling{-}

Beginning with a training split of 70%, we will build a random forest model and check the accuracy of our model against the 30% testing partition. Then we will use this split to build a simple random forest model before moving on to build models with hyperparameter tuning.

```{r, results='hide', warning=FALSE}
h2o.init()
attritionTrainHF <- as.h2o(creditData)
y <- "Attrition_Flag"
x <- setdiff(colnames(attritionTrainHF), y)
sframe <- h2o.splitFrame(attritionTrainHF, ratios = .7, seed = 1212)
train <- sframe[[1]]
test <- sframe[[2]]
summary(train$Attrition_Flag, exact_quantiles = TRUE)
rfmodel <- h2o.randomForest(x = x,
                y = y,
                training_frame = train,
                validation_frame = test)
performanceRF <- h2o.performance(rfmodel, valid = TRUE)
```

Now that we have built a random forest model and calculated the model performance, let's take a look at the confusion matrix and model accuracy.

```{r}
h2o.confusionMatrix(performanceRF)
h2o.logloss(performanceRF)
```
Using our test training split of 70/30, our random forest model was able to predict whether a customer will attrite with an accuracy level of 89.154% and whether a customer will remain in the program with an accuracy level of 97.516%. This brings us to an overall accuracy level of 96.195% and a log loss of .1370294. This is a fairly high accuracy level, though we may be able to improve it using a different modeling method. Let's predict whether a customer will attrite using an automatic machine learning function from the 'h2o' library. We will use a maximum run time of 300 seconds and three folds to find a model with potentially lower log loss than the random forest we created. Once we have run our automatic machine learning model, we can extract the most accurate model and see which model parameters have the highest relative importance.

```{r, results='hide', warning=FALSE}
automl_model <- h2o.automl(x = x, 
                           y = y,
                           training_frame = train,
                           nfolds = 3,
                           max_runtime_secs = 300,
                           sort_metric = "mean_per_class_error",
                           seed = 1212)
lboard <- automl_model@leaderboard
model_leader <- automl_model@leader
head(lboard)

topStats <- h2o.performance(model_leader, test)
summary(model_leader)
lboard
```
Our best performing model on the test data is an extreme gradient boosted machine model, with an AUC of `r topStats@metrics$AUC`, a very high score. We also have an R-squared value of `r topStats@metrics$r2`, a relatively high score. The log loss improved by a decent margin, with a log loss value of `r topStats@metrics$logloss`. Let's also take a look at which variables the model placed the highest relative importance on.

```{r}
model_leader@model$variable_importances[1:5,1:4]
```

`r model_leader@model$variable_importances[1,1]`
Our best model puts a high relative importance on `r model_leader@model$variable_importances[1,1]`, `r model_leader@model$variable_importances[2,1]`, `r model_leader@model$variable_importances[3,1]`, `r model_leader@model$variable_importances[4,1]`, and `r model_leader@model$variable_importances[5,1]`. There are other important variables but when initially taking a look at the data we saw some of these top five have pretty significant differences between their averages for attrited versus existing customers. It also appears that the demographics, such as gender and education level, had very little impact on whether or not a customer might attrite.

It is not surprising that the gradient boosted model is much better than the random forest, though there were other similarly strong models, such as a normal gradient boosted model and a stacked ensemble. Based on the different model rankings though, it appears that either an extreme gradient boosted model or normal gradient boosted model would yield the highest accuracy on validation data.



# Conclusion{-}
Many of the models that we developed through this automatic method would show similar model statistics and performance on the test data. If we were given a larger data set we may be able to more accurately tune the hyperparameters for predicting which customers may be leaving the program. The results from this set look very promising though and I would be interested to see the accuracy of the model when applied to future program members and whether or not the high levels of accuracy hold.


































